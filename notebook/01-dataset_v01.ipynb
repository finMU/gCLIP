{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLIP - Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <script type=\"application/javascript\" id=\"jupyter_black\">\n",
       "                (function() {\n",
       "                    if (window.IPython === undefined) {\n",
       "                        return\n",
       "                    }\n",
       "                    var msg = \"WARNING: it looks like you might have loaded \" +\n",
       "                        \"jupyter_black in a non-lab notebook with \" +\n",
       "                        \"`is_lab=True`. Please double check, and if \" +\n",
       "                        \"loading with `%load_ext` please review the README!\"\n",
       "                    console.log(msg)\n",
       "                    alert(msg)\n",
       "                })()\n",
       "                </script>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext jupyter_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset - Line by Line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 image - transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform(img_size: int, max_pixel_value: float = 255.0, stage: str = \"train\"):\n",
    "    if stage == \"train\":\n",
    "        transform = A.Compose(\n",
    "            [\n",
    "                A.Resize(img_size, img_size, always_apply=True),\n",
    "                A.Normalize(max_pixel_value=max_pixel_value, always_apply=True),\n",
    "                ToTensorV2(),\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        transform = A.Compose(\n",
    "            [\n",
    "                A.Resize(img_size, img_size, always_apply=True),\n",
    "                A.Normalize(max_pixel_value=max_pixel_value, always_apply=True),\n",
    "                ToTensorV2(),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    return transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = get_transform(img_size=224)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 text - tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_name = \"distilbert-base-uncased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = \"../data/Flickr-8k/Images/\"\n",
    "caption_path = \"../data/Flickr-8k/captions.txt\"\n",
    "info_df = pd.read_csv(caption_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A child in a pink dress is climbing up a set o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A girl going into a wooden building .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl climbing into a wooden playhouse .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl climbing the stairs to her playh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1000268201_693b08cb0e.jpg</td>\n",
       "      <td>A little girl in a pink dress going into a woo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       image  \\\n",
       "0  1000268201_693b08cb0e.jpg   \n",
       "1  1000268201_693b08cb0e.jpg   \n",
       "2  1000268201_693b08cb0e.jpg   \n",
       "3  1000268201_693b08cb0e.jpg   \n",
       "4  1000268201_693b08cb0e.jpg   \n",
       "\n",
       "                                             caption  \n",
       "0  A child in a pink dress is climbing up a set o...  \n",
       "1              A girl going into a wooden building .  \n",
       "2   A little girl climbing into a wooden playhouse .  \n",
       "3  A little girl climbing the stairs to her playh...  \n",
       "4  A little girl in a pink dress going into a woo...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 img & text -  preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = random.choice(range(len(info_df)))\n",
    "\n",
    "fname = info_df.iloc[idx][\"image\"]\n",
    "caption = info_df.iloc[idx][\"caption\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 200\n",
    "\n",
    "# encoded_caption = tokenizer(\n",
    "#     caption, padding=True, truncation=True, max_length=max_length\n",
    "# )\n",
    "\n",
    "captions = info_df[\"caption\"].tolist()\n",
    "\n",
    "encoded_captions = tokenizer(\n",
    "    captions, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([40455, 42])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_captions[\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = os.path.join(img_dir, fname)\n",
    "\n",
    "img = cv2.imread(img_path)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "img = transform(image=img)[\"image\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = {k: v[idx] for k, v in encoded_captions.items()}\n",
    "item[\"image\"] = img\n",
    "item[\"caption\"] = caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([  101,  1037,  2158,  1998,  1037,  2450,  2265,  2037, 11937,  3406,\n",
       "         29099,  8072,  2006,  2037, 12150,   102,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'image': tensor([[[ 1.9235,  1.9235,  1.9235,  ...,  1.1872,  1.1700,  1.1529],\n",
       "          [ 1.9235,  1.9235,  1.9235,  ...,  1.1700,  1.1700,  1.1529],\n",
       "          [ 1.9235,  1.9235,  1.9235,  ...,  1.1700,  1.1700,  1.1529],\n",
       "          ...,\n",
       "          [-1.7069, -1.6898, -1.7412,  ...,  0.5193,  0.7419,  0.8104],\n",
       "          [-1.7240, -1.7240, -1.6555,  ...,  0.8447,  0.7933,  0.8618],\n",
       "          [-1.7754, -1.6898, -1.6042,  ...,  0.9303,  0.9303,  0.7248]],\n",
       " \n",
       "         [[ 1.9909,  1.9909,  1.9909,  ...,  1.5182,  1.5007,  1.4832],\n",
       "          [ 1.9909,  1.9909,  1.9909,  ...,  1.5007,  1.5007,  1.4832],\n",
       "          [ 1.9909,  1.9909,  1.9909,  ...,  1.5007,  1.5007,  1.4832],\n",
       "          ...,\n",
       "          [-1.9657, -1.9307, -1.9132,  ...,  0.6254,  0.8354,  0.9230],\n",
       "          [-1.9832, -1.9657, -1.8606,  ...,  0.9405,  0.8704,  0.9755],\n",
       "          [-1.9832, -1.9482, -1.8431,  ...,  0.9930,  1.0105,  0.8354]],\n",
       " \n",
       "         [[ 2.2043,  2.2043,  2.2043,  ...,  1.9428,  1.9254,  1.8905],\n",
       "          [ 2.2043,  2.2043,  2.2043,  ...,  1.9254,  1.9254,  1.9080],\n",
       "          [ 2.2043,  2.2043,  2.2043,  ...,  1.9254,  1.9254,  1.9080],\n",
       "          ...,\n",
       "          [-1.8044, -1.7696, -1.7870,  ...,  1.2108,  1.4200,  1.5071],\n",
       "          [-1.7870, -1.7870, -1.7522,  ...,  1.5245,  1.4897,  1.5768],\n",
       "          [-1.7870, -1.7870, -1.7347,  ...,  1.6117,  1.6291,  1.4722]]]),\n",
       " 'caption': 'A man and a woman show their tatooed hearts on their wrists'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset - Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        img_dir: str,\n",
    "        tokenizer,\n",
    "        transform,\n",
    "        txt_max_length: int = 200,\n",
    "    ):\n",
    "        self.img_dir = img_dir\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transform = transform\n",
    "        self.txt_max_length = txt_max_length\n",
    "\n",
    "        # dataframe\n",
    "        self.data = df\n",
    "\n",
    "        # encoded_captions\n",
    "        captions = self.data[\"caption\"].tolist()\n",
    "        self.encoded_captions = self.tokenizer(\n",
    "            captions,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=self.txt_max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fname = self.data.iloc[idx][\"image\"]\n",
    "        caption = self.data.iloc[idx][\"caption\"]\n",
    "\n",
    "        # txt prep\n",
    "        item = {k: v[idx] for k, v in self.encoded_captions.items()}\n",
    "\n",
    "        # img prep\n",
    "        img_path = os.path.join(self.img_dir, fname)\n",
    "\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = self.transform(image=img)[\"image\"]\n",
    "\n",
    "        item[\"image\"] = img\n",
    "        item[\"caption\"] = caption\n",
    "\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/Flickr-8k/captions.txt\"\n",
    "img_dir = \"../data/Flickr-8k/Images\"\n",
    "tokenizer_name = \"distilbert-base-uncased\"\n",
    "img_size = 224\n",
    "txt_max_length = 200\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "transform = get_transform(img_size=img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.2\n",
    "val_size = 0.2\n",
    "\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=test_size, shuffle=True)\n",
    "train_df, val_df = train_test_split(train_df, test_size=test_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CLIPDataset(train_df, img_dir, tokenizer, transform, txt_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 2093, 2111,  ...,    0,    0,    0],\n",
       "         [ 101, 2048, 2312,  ...,    0,    0,    0],\n",
       "         [ 101, 2048, 6077,  ...,    0,    0,    0],\n",
       "         ...,\n",
       "         [ 101, 2048, 4268,  ...,    0,    0,    0],\n",
       "         [ 101, 1037, 2158,  ...,    0,    0,    0],\n",
       "         [ 101, 1037, 2450,  ...,    0,    0,    0]]),\n",
       " 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0],\n",
       "         [1, 1, 1,  ..., 0, 0, 0]]),\n",
       " 'image': tensor([[[[-1.8782, -1.8268, -1.8268,  ..., -1.5699, -1.7069, -1.8097],\n",
       "           [-1.8610, -1.8439, -1.8097,  ..., -1.7583, -1.5357, -1.7925],\n",
       "           [-1.8610, -1.8268, -1.8268,  ..., -1.2617, -1.6042, -1.7412],\n",
       "           ...,\n",
       "           [-2.0665, -2.0665, -2.0665,  ..., -2.1179, -2.1179, -2.1179],\n",
       "           [-2.0665, -2.0494, -2.0665,  ..., -2.1179, -2.1179, -2.1179],\n",
       "           [-2.0837, -2.0665, -2.0837,  ..., -2.1179, -2.0837, -2.1179]],\n",
       " \n",
       "          [[ 0.4153,  0.4678,  0.4678,  ...,  1.5007,  1.3957,  1.3606],\n",
       "           [ 0.4328,  0.4503,  0.4853,  ...,  1.9384,  1.4482,  1.3431],\n",
       "           [ 0.4328,  0.4678,  0.4678,  ...,  1.9209,  1.4132,  1.3606],\n",
       "           ...,\n",
       "           [-0.3025, -0.2675, -0.2500,  ..., -0.1275, -0.1099,  0.0476],\n",
       "           [-0.3025, -0.2675, -0.2675,  ..., -0.0924, -0.0224,  0.0476],\n",
       "           [-0.3025, -0.2850, -0.2850,  ..., -0.0049,  0.0126, -0.0049]],\n",
       " \n",
       "          [[ 2.0997,  2.1520,  2.1520,  ...,  2.6051,  2.5877,  2.5877],\n",
       "           [ 2.1171,  2.1346,  2.1694,  ...,  2.6226,  2.6400,  2.5703],\n",
       "           [ 2.1171,  2.1520,  2.1520,  ...,  2.6051,  2.5877,  2.6051],\n",
       "           ...,\n",
       "           [ 1.1759,  1.1934,  1.2457,  ...,  1.0888,  1.0888,  1.1934],\n",
       "           [ 1.1585,  1.1934,  1.1934,  ...,  1.1062,  1.1411,  1.1934],\n",
       "           [ 1.1585,  1.1759,  1.1759,  ...,  1.1411,  1.1759,  1.1585]]],\n",
       " \n",
       " \n",
       "         [[[ 0.8104,  0.7933,  0.8618,  ...,  0.8104,  0.8276,  0.8276],\n",
       "           [ 0.8276,  0.8276,  0.9132,  ...,  0.8104,  0.8104,  0.7933],\n",
       "           [ 0.9303,  0.9132,  0.9474,  ...,  0.8447,  0.8104,  0.8104],\n",
       "           ...,\n",
       "           [ 0.5022,  0.5364,  0.5878,  ...,  0.3994,  0.3823,  0.3652],\n",
       "           [ 0.4851,  0.5707,  0.4679,  ...,  0.3994,  0.3652,  0.3823],\n",
       "           [ 0.5022,  0.5707,  0.5193,  ...,  0.3823,  0.4166,  0.4679]],\n",
       " \n",
       "          [[ 0.9755,  0.9755,  0.9755,  ...,  0.9405,  0.9580,  0.9580],\n",
       "           [ 1.0105,  0.9930,  1.0805,  ...,  0.9755,  0.9755,  0.9580],\n",
       "           [ 1.0805,  1.1331,  1.0980,  ...,  1.0105,  0.9755,  0.9755],\n",
       "           ...,\n",
       "           [ 0.6604,  0.6779,  0.6604,  ...,  0.5553,  0.5028,  0.5203],\n",
       "           [ 0.6779,  0.6779,  0.6954,  ...,  0.4853,  0.5203,  0.4853],\n",
       "           [ 0.6604,  0.6604,  0.6954,  ...,  0.4678,  0.5203,  0.5203]],\n",
       " \n",
       "          [[ 1.1759,  1.1934,  1.2457,  ...,  1.2631,  1.2631,  1.1759],\n",
       "           [ 1.2631,  1.2457,  1.3328,  ...,  1.2631,  1.2282,  1.2457],\n",
       "           [ 1.3851,  1.3851,  1.3502,  ...,  1.2631,  1.2282,  1.2805],\n",
       "           ...,\n",
       "           [ 0.9494,  0.9842,  0.9494,  ...,  0.9145,  0.9145,  0.8797],\n",
       "           [ 0.9668,  1.0191,  0.9668,  ...,  0.8622,  0.8274,  0.8274],\n",
       "           [ 0.9668,  0.9842,  0.9494,  ...,  0.8274,  0.8448,  0.8622]]],\n",
       " \n",
       " \n",
       "         [[[ 0.1083,  0.1426,  0.1768,  ..., -0.0458,  0.5193,  0.3309],\n",
       "           [ 0.1426,  0.1083,  0.0741,  ..., -0.0458,  0.8447,  1.2214],\n",
       "           [ 0.0912,  0.0912,  0.0056,  ...,  1.0844,  1.5297,  0.2282],\n",
       "           ...,\n",
       "           [ 0.0741,  0.1426,  0.0056,  ...,  0.3994, -0.2684,  0.4337],\n",
       "           [ 0.1597,  0.0569,  0.1597,  ...,  0.5536,  0.0227,  0.4851],\n",
       "           [ 0.2111,  0.1426,  0.1768,  ...,  0.3309,  0.6392,  0.2282]],\n",
       " \n",
       "          [[-0.2325, -0.1800, -0.1099,  ..., -0.5651, -0.2150, -0.0749],\n",
       "           [-0.1975, -0.2150, -0.1450,  ..., -0.5826,  0.5728,  1.2206],\n",
       "           [-0.2325, -0.2500, -0.2850,  ...,  1.0280,  1.6408,  0.4853],\n",
       "           ...,\n",
       "           [-0.1975, -0.0924, -0.1975,  ..., -0.0224, -0.7577, -0.0574],\n",
       "           [-0.0924, -0.2150, -0.0574,  ...,  0.0826, -0.4601, -0.0049],\n",
       "           [-0.0399, -0.1625, -0.0924,  ..., -0.1800,  0.1702, -0.2850]],\n",
       " \n",
       "          [[-0.3578, -0.3578, -0.3055,  ..., -1.0201, -0.7064, -0.7936],\n",
       "           [-0.3578, -0.3753, -0.3578,  ..., -1.0550,  0.1651,  1.1934],\n",
       "           [-0.4101, -0.3927, -0.4624,  ...,  0.7402,  1.6465,  0.9145],\n",
       "           ...,\n",
       "           [-0.2881, -0.1487, -0.2358,  ..., -0.3753, -1.0201, -0.4973],\n",
       "           [-0.2184, -0.2881, -0.1312,  ..., -0.3753, -0.8110, -0.4450],\n",
       "           [-0.1835, -0.2532, -0.1835,  ..., -0.6890, -0.2010, -0.6715]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[ 1.6153,  1.6495,  1.6324,  ..., -0.0801, -0.1314, -0.1486],\n",
       "           [ 1.5982,  1.6324,  1.6495,  ..., -0.1999, -0.0629, -0.1314],\n",
       "           [ 1.6495,  1.6667,  1.6153,  ..., -0.3198, -0.0458, -0.0972],\n",
       "           ...,\n",
       "           [-0.7650, -0.6623, -0.8678,  ..., -0.9363, -0.9877, -1.0219],\n",
       "           [-0.9192, -0.9020, -0.7137,  ..., -1.0390, -1.0562, -1.1075],\n",
       "           [-0.7822, -0.7479, -0.6965,  ..., -0.9877, -1.0390, -1.0733]],\n",
       " \n",
       "          [[ 1.9384,  1.9384,  1.8859,  ...,  0.3627,  0.3102,  0.2927],\n",
       "           [ 1.8859,  1.9384,  1.9734,  ...,  0.2052,  0.3277,  0.2752],\n",
       "           [ 1.8859,  1.9384,  1.9034,  ...,  0.0651,  0.3978,  0.3452],\n",
       "           ...,\n",
       "           [-0.7927, -0.7752, -0.9503,  ..., -1.0903, -1.2129, -1.2129],\n",
       "           [-1.1078, -0.9503, -0.8102,  ..., -0.9328, -1.1604, -1.1429],\n",
       "           [-0.8452, -0.7577, -0.7752,  ..., -0.9328, -1.0728, -1.0553]],\n",
       " \n",
       "          [[ 2.4134,  2.4483,  2.4134,  ...,  0.9145,  0.8448,  0.7576],\n",
       "           [ 2.4134,  2.3263,  2.3437,  ...,  0.8797,  0.9145,  0.8622],\n",
       "           [ 2.3786,  2.3611,  2.3960,  ...,  0.6008,  0.9668,  0.9494],\n",
       "           ...,\n",
       "           [-0.5844, -0.6367, -0.8110,  ..., -0.9504, -0.8633, -1.1247],\n",
       "           [-0.7761, -0.3404, -0.4275,  ..., -0.7761, -0.9504, -1.1770],\n",
       "           [-0.6193, -0.5495, -0.4798,  ..., -0.8110, -0.7936, -0.8807]]],\n",
       " \n",
       " \n",
       "         [[[-1.8610, -2.0494, -1.8953,  ..., -2.0665, -2.0665, -2.0837],\n",
       "           [-1.7754, -1.9809, -1.9467,  ..., -2.0837, -2.1008, -2.1008],\n",
       "           [-1.7412, -1.9124, -1.9809,  ..., -2.0837, -2.0665, -2.0837],\n",
       "           ...,\n",
       "           [-1.7069, -1.7069, -1.7412,  ..., -1.2617, -0.9705, -1.3130],\n",
       "           [-1.7240, -1.7412, -1.7069,  ..., -0.6281, -1.1589, -1.1760],\n",
       "           [-1.7412, -1.7412, -1.6555,  ..., -1.2617, -1.0733, -1.2788]],\n",
       " \n",
       "          [[-1.7731, -1.9657, -1.8081,  ..., -1.9832, -1.9832, -2.0007],\n",
       "           [-1.6856, -1.8957, -1.8606,  ..., -2.0007, -2.0182, -2.0182],\n",
       "           [-1.6506, -1.8256, -1.8957,  ..., -2.0007, -1.9832, -2.0007],\n",
       "           ...,\n",
       "           [-1.6155, -1.6155, -1.6506,  ..., -0.5651, -0.6702, -1.0203],\n",
       "           [-1.6331, -1.6506, -1.6155,  ..., -0.3725, -0.7927, -0.7752],\n",
       "           [-1.6506, -1.6506, -1.5630,  ..., -0.9853, -0.6527, -1.0028]],\n",
       " \n",
       "          [[-1.5430, -1.7347, -1.5779,  ..., -1.7522, -1.7522, -1.7696],\n",
       "           [-1.4559, -1.6650, -1.6302,  ..., -1.7696, -1.7870, -1.7870],\n",
       "           [-1.4210, -1.5953, -1.6650,  ..., -1.7696, -1.7522, -1.7696],\n",
       "           ...,\n",
       "           [-1.4384, -1.3861, -1.4210,  ..., -0.9156, -0.7936, -1.0724],\n",
       "           [-1.4384, -1.4559, -1.4210,  ..., -0.3578, -1.0550, -0.8458],\n",
       "           [-1.4559, -1.4559, -1.3687,  ..., -1.0201, -0.8633, -1.0724]]],\n",
       " \n",
       " \n",
       "         [[[ 1.1187,  1.2728,  1.3070,  ..., -1.7583, -1.8097, -1.8097],\n",
       "           [ 1.1358,  0.9817,  1.1872,  ..., -1.7925, -1.8610, -1.8439],\n",
       "           [ 1.3070,  1.3413,  1.3755,  ..., -1.8782, -1.8268, -1.8097],\n",
       "           ...,\n",
       "           [-0.7137, -0.5253, -0.3198,  ..., -1.8610, -1.6384, -1.3987],\n",
       "           [-0.7137, -0.2513,  0.0227,  ..., -1.8439, -1.5185, -1.3130],\n",
       "           [-0.3198,  0.0398, -0.0801,  ..., -1.8268, -1.6042, -1.3644]],\n",
       " \n",
       "          [[ 0.3277,  0.2227,  0.3102,  ..., -1.6681, -1.6856, -1.6856],\n",
       "           [ 0.2402,  0.2927,  0.2752,  ..., -1.7206, -1.6856, -1.6681],\n",
       "           [ 1.0980,  1.0805,  1.1856,  ..., -1.7381, -1.7206, -1.7031],\n",
       "           ...,\n",
       "           [-0.4076, -0.2150,  0.0826,  ..., -1.8431, -1.7731, -1.6856],\n",
       "           [-0.3901, -0.0049,  0.3803,  ..., -1.8256, -1.7731, -1.6856],\n",
       "           [-0.0224,  0.3803,  0.2752,  ..., -1.8957, -1.7381, -1.6856]],\n",
       " \n",
       "          [[ 0.0953,  0.0779,  0.1825,  ..., -1.3339, -1.3687, -1.3687],\n",
       "           [ 0.0953,  0.1302,  0.0605,  ..., -1.4036, -1.3861, -1.3687],\n",
       "           [ 0.9842,  1.0365,  1.0191,  ..., -1.4733, -1.4036, -1.4036],\n",
       "           ...,\n",
       "           [-0.3753, -0.0092,  0.3742,  ..., -1.6650, -1.5430, -1.4210],\n",
       "           [-0.3578,  0.2522,  0.5834,  ..., -1.6476, -1.4907, -1.4036],\n",
       "           [-0.0441,  0.5136,  0.3568,  ..., -1.6824, -1.5256, -1.4210]]]]),\n",
       " 'caption': ['Three people scuba dive under the sea .',\n",
       "  'Two large black dogs run through the snow .',\n",
       "  'Two dogs on a wet , sandy track .',\n",
       "  'A man in a uniform waves while holding a banner during an event .',\n",
       "  'A football player being tackled from behind by another player from a different team in front of a crowd .',\n",
       "  'A couple of people sit outdoors at a table with an umbrella and talk .',\n",
       "  'Two children playing in a spray of water .',\n",
       "  'One black dog is jumping off the snow filled ground .',\n",
       "  'A woman plays guitar by a round light .',\n",
       "  'Boy in a blue uniform playing soccer .',\n",
       "  'The girl min the pink top is looking at the man who is wearing orange boots and patterned pants .',\n",
       "  'The boy in the gray t-shirt is standing next to the girl in the green dress and cowboy hat on the beach .',\n",
       "  'Two men shaking hands and smiling at the camera , while a third man stands behind them .',\n",
       "  'A dog leaps over a barrier .',\n",
       "  'A girl in colorful clothing is jumping on a bed with a color-block quilt .',\n",
       "  'A group of people sit in the snow overlooking a mountain scene .',\n",
       "  'Four people taking in front of a bus .',\n",
       "  'Dog and bird together in water .',\n",
       "  'Two people backlit with light sky .',\n",
       "  'A dog is running .',\n",
       "  'The man in a white shirt is skateboarding .',\n",
       "  'Three women in elaborate costumes hold up \" I Love You \" cards .',\n",
       "  'Two dogs play rough with each other indoors .',\n",
       "  'A man walks down an empty street next to a market .',\n",
       "  'Children play with large hoops .',\n",
       "  'The woman in the red shirt is holding up a peace sign with her fingers with a red banner behind her .',\n",
       "  'a brown dog is jumping up at a woman in a black coat .',\n",
       "  'A group of young men in sports uniforms from two opposing teams are in vigorous play on a grassy field .',\n",
       "  'A guy grinds a windowsill near an old windmill',\n",
       "  'Two kids ride their bikes by the ocean , in the evening .',\n",
       "  'A man in warm clothes swinging on monkey bars at night .',\n",
       "  'A woman holding a newspaper .']}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPDataModule:\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path: str,\n",
    "        img_dir: str,\n",
    "        tokenizer_name: str,\n",
    "        img_size: int = 224,\n",
    "        txt_max_length: int = 200,\n",
    "        val_size: float = 0.2,\n",
    "        test_size: float = 0.2,\n",
    "        batch_size: int = 32,\n",
    "        num_workers: int = 4,\n",
    "    ):\n",
    "        self.data_path = data_path\n",
    "        self.img_dir = img_dir\n",
    "        self.tokenizer_name = tokenizer_name\n",
    "        self.img_size = img_size\n",
    "        self.txt_max_length = txt_max_length\n",
    "        self.val_size = val_size\n",
    "        self.test_size = test_size\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "        self.setup()\n",
    "\n",
    "    def setup(self):\n",
    "        # load data\n",
    "        self.df = pd.read_csv(self.data_path)\n",
    "\n",
    "        # tokenizer & transform\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.tokenizer_name)\n",
    "        self.train_transform = get_transform(img_size=self.img_size, stage=\"train\")\n",
    "        self.test_transform = get_transform(img_size=self.img_size, stage=\"test\")\n",
    "\n",
    "        # train/val/test split\n",
    "        train_df, test_df = train_test_split(\n",
    "            self.df, test_size=self.test_size, shuffle=True\n",
    "        )\n",
    "        train_df, val_df = train_test_split(\n",
    "            train_df, test_size=self.test_size, shuffle=True\n",
    "        )\n",
    "\n",
    "        # train/val/test set\n",
    "        self.trainset = CLIPDataset(\n",
    "            train_df,\n",
    "            self.img_dir,\n",
    "            self.tokenizer,\n",
    "            self.train_transform,\n",
    "            self.txt_max_length,\n",
    "        )\n",
    "        self.valset = CLIPDataset(\n",
    "            val_df,\n",
    "            self.img_dir,\n",
    "            self.tokenizer,\n",
    "            self.test_transform,\n",
    "            self.txt_max_length,\n",
    "        )\n",
    "        self.testset = CLIPDataset(\n",
    "            test_df,\n",
    "            self.img_dir,\n",
    "            self.tokenizer,\n",
    "            self.test_transform,\n",
    "            self.txt_max_length,\n",
    "        )\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.trainset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.valset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=True,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.testset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm_params = {\n",
    "    \"data_path\": \"../data/Flickr-8k/captions.txt\",\n",
    "    \"img_dir\": \"../data/Flickr-8k/Images\",\n",
    "    \"tokenizer_name\": \"distilbert-base-uncased\",\n",
    "    \"img_size\": 224,\n",
    "    \"txt_max_length\": 200,\n",
    "    \"val_size\": 0.2,\n",
    "    \"test_size\": 0.2,\n",
    "    \"batch_size\": 32,\n",
    "    \"num_workers\": 4,\n",
    "}\n",
    "\n",
    "dm = CLIPDataModule(**dm_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch = next(iter(dm.train_dataloader()))\n",
    "val_batch = next(iter(dm.val_dataloader()))\n",
    "test_batch = next(iter(dm.test_dataloader()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.18 ('gclip')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "15b6d287f2a01b064242a5db7155158bf7f3638b6b99c51e9d30d6c2bf5cc073"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
