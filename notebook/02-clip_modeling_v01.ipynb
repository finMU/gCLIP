{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLIP Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <script type=\"application/javascript\" id=\"jupyter_black\">\n",
       "                (function() {\n",
       "                    if (window.IPython === undefined) {\n",
       "                        return\n",
       "                    }\n",
       "                    var msg = \"WARNING: it looks like you might have loaded \" +\n",
       "                        \"jupyter_black in a non-lab notebook with \" +\n",
       "                        \"`is_lab=True`. Please double check, and if \" +\n",
       "                        \"loading with `%load_ext` please review the README!\"\n",
       "                    console.log(msg)\n",
       "                    alert(msg)\n",
       "                })()\n",
       "                </script>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext jupyter_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import timm\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataset.datamodule import CLIPDataModule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. CLIP DataModule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "dm_params = {\n",
    "    \"data_path\": \"../data/captions.csv\",\n",
    "    \"img_dir\": \"../data/images/\",\n",
    "    \"tokenizer_name\": \"bert-base-uncased\",\n",
    "    \"img_size\": 224,\n",
    "    \"txt_max_length\": 200,\n",
    "    \"val_size\": 0.2,\n",
    "    \"test_size\": 0.2,\n",
    "    \"batch_size\": 32,\n",
    "    \"num_workers\": 4,\n",
    "}\n",
    "\n",
    "dm = CLIPDataModule(**dm_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dm.train_dataloader()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Image Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, model_name: str, use_pretrained: bool = True, is_trainable: bool = True\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model_name = model_name\n",
    "        self.use_pretrained = use_pretrained\n",
    "        self.is_trainable = is_trainable\n",
    "\n",
    "        # img encoer init\n",
    "        self.model = timm.create_model(\n",
    "            model_name, num_classes=0, global_pool=\"avg\", pretrained=use_pretrained\n",
    "        )\n",
    "\n",
    "        if not self.is_trainable:\n",
    "            for parameter in self.model.parameters():\n",
    "                parameter.requires_grad = self.is_trainable\n",
    "\n",
    "    def forward(self, img: torch.Tensor):\n",
    "        return self.model(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Text Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, model_name: str, use_pretrained: bool = True, is_trainable: bool = True\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model_name = model_name\n",
    "        self.use_pretrained = use_pretrained\n",
    "        self.is_trainable = is_trainable\n",
    "        self.cls_token_idx = 0\n",
    "\n",
    "        if use_pretrained:\n",
    "            self.model = AutoModel.from_pretrained(model_name)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        if not self.is_trainable:\n",
    "            for parameter in self.model.parameters():\n",
    "                parameter.requires_grad = self.is_trainable\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        output = self.model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = output.last_hidden_state\n",
    "        return last_hidden_state[:, self.cls_token_idx, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Projection Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionHead(nn.Module):\n",
    "    \"\"\"\n",
    "    ref:  https://github.com/h-albert-lee/G-CLIP/blob/master/modules.py\n",
    "\n",
    "    TODO: img encoder에 layer_norm 적절한지?\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dim: int, projection_dim: int, dropout: float):\n",
    "        super().__init__()\n",
    "\n",
    "        self.projection = nn.Linear(embedding_dim, projection_dim)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.fc = nn.Linear(projection_dim, projection_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(projection_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        projected = self.projection(x)\n",
    "        x = self.gelu(projected)\n",
    "        x = self.fc(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x + projected\n",
    "        x = self.layer_norm(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. CLIP Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Line by Line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_trainable = True\n",
    "use_pretrained = True\n",
    "\n",
    "# img encoder\n",
    "img_model_name = \"vit_base_patch16_224\"\n",
    "img_embedding = 768\n",
    "\n",
    "# text encoder\n",
    "text_model_name = \"bert-base-uncased\"\n",
    "text_embedding = 768\n",
    "\n",
    "# projection head\n",
    "projection_dim = 256\n",
    "dropout = 0.1\n",
    "\n",
    "# clip\n",
    "temperature = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_encoder = ImageEncoder(img_model_name)\n",
    "text_encoder = TextEncoder(text_model_name)\n",
    "\n",
    "img_projection = ProjectionHead(\n",
    "    embedding_dim=img_embedding, projection_dim=projection_dim, dropout=dropout\n",
    ")\n",
    "text_projection = ProjectionHead(\n",
    "    embedding_dim=text_embedding, projection_dim=projection_dim, dropout=dropout\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img & text features from encoder\n",
    "img_features = img_encoder(batch[\"image\"])\n",
    "text_features = text_encoder(batch[\"input_ids\"], batch[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# img & text embedding from projection head\n",
    "img_embeddings = img_projection(img_features)\n",
    "text_embeddings = text_projection(text_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the Loss\n",
    "logits = (text_embeddings @ img_embeddings.T) / temperature\n",
    "images_similarity = img_embeddings @ img_embeddings.T\n",
    "texts_similarity = text_embeddings @ text_embeddings.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = F.softmax((images_similarity + texts_similarity) / 2 * temperature, dim=-1)\n",
    "texts_loss = F.cross_entropy(logits, targets, reduction=\"none\")\n",
    "images_loss = F.cross_entropy(logits.T, targets.T, reduction=\"none\")\n",
    "loss = (images_loss + texts_loss) / 2.0  # shape: (batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(13.3602, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 CLIP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIP(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        img_model_name: str,\n",
    "        text_model_name: str,\n",
    "        temperature: float,\n",
    "        img_embedding: int,\n",
    "        text_embedding: int,\n",
    "        projection_dim: int,\n",
    "        dropout: float,\n",
    "        is_trainable: bool = True,\n",
    "        use_pretrained: bool = True,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.img_model_name = img_model_name\n",
    "        self.text_model_name = text_model_name\n",
    "        self.temperature = temperature\n",
    "\n",
    "        self.img_encoder = ImageEncoder(img_model_name, use_pretrained, is_trainable)\n",
    "        self.text_encoder = TextEncoder(text_model_name, use_pretrained, is_trainable)\n",
    "\n",
    "        self.img_projection = ProjectionHead(\n",
    "            embedding_dim=img_embedding, projection_dim=projection_dim, dropout=dropout\n",
    "        )\n",
    "        self.text_projection = ProjectionHead(\n",
    "            embedding_dim=text_embedding, projection_dim=projection_dim, dropout=dropout\n",
    "        )\n",
    "\n",
    "    def forward(self, batch: dict[str, torch.Tensor]):\n",
    "        # img & text features from encoder\n",
    "        img_features = self.img_encoder(batch[\"image\"])\n",
    "        text_features = self.text_encoder(batch[\"input_ids\"], batch[\"attention_mask\"])\n",
    "\n",
    "        # img & text embedding from projection head\n",
    "        img_embeddings = self.img_projection(img_features)\n",
    "        text_embeddings = self.text_projection(text_features)\n",
    "\n",
    "        # Calculating the Loss\n",
    "        logits = (text_embeddings @ img_embeddings.T) / self.temperature\n",
    "        imgs_similarity = img_embeddings @ img_embeddings.T\n",
    "        texts_similarity = text_embeddings @ text_embeddings.T\n",
    "\n",
    "        targets = F.softmax(\n",
    "            (imgs_similarity + texts_similarity) / 2 * self.temperature, dim=-1\n",
    "        )\n",
    "        texts_loss = F.cross_entropy(logits, targets, reduction=\"none\")\n",
    "        imgs_loss = F.cross_entropy(logits.T, targets.T, reduction=\"none\")\n",
    "        loss = (imgs_loss + texts_loss) / 2.0\n",
    "\n",
    "        return {\n",
    "            \"loss\": loss.mean(),\n",
    "            \"img_embeddings\": img_embeddings,\n",
    "            \"text_embeddings\": text_embeddings,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    \"is_trainable\": True,\n",
    "    \"use_pretrained\": True,\n",
    "    # img encoder\n",
    "    \"img_model_name\": \"vit_base_patch16_224\",\n",
    "    \"img_embedding\": 768,\n",
    "    # text encoder\n",
    "    \"text_model_name\": \"bert-base-uncased\",\n",
    "    \"text_embedding\": 768,\n",
    "    # projection head\n",
    "    \"projection_dim\": 256,\n",
    "    \"dropout\": 0.1,\n",
    "    # clip\n",
    "    \"temperature\": 1.0,\n",
    "}\n",
    "\n",
    "model = CLIP(**model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(19.7848, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[\"loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 256])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[\"img_embeddings\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 256])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[\"text_embeddings\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.18 ('gclip')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "15b6d287f2a01b064242a5db7155158bf7f3638b6b99c51e9d30d6c2bf5cc073"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
